{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CLEAN THE NOISE"
      ],
      "metadata": {
        "id": "alHHznv1JEdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnglBhyTvbgQ",
        "outputId": "bef36a7a-157b-4fe4-b558-78eb247c606f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to merge the text files\n",
        "\n",
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/TextMining/Missing_Data'\n",
        "output_file = '/content/drive/MyDrive/TextMining/Output/merged.txt'\n",
        "\n",
        "with open(output_file, 'w') as outfile:\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, 'r') as infile:\n",
        "                outfile.write(infile.read() + '\\n')"
      ],
      "metadata": {
        "id": "RBB9kHiZdVHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRehCBZdE9TI"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"PubMed_Abstract_TJA_Denoising.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1qC-y49_YjuieokZDboosi21qesfElcMH\n",
        "\"\"\"\n",
        "\n",
        "############################################################################\n",
        "# PubMed Abstracts: Denoising\n",
        "############################################################################\n",
        "\n",
        "# Mounting your Google Drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import re\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/TextMining/Output/merged.txt\", 'r', encoding=\"utf-8\")\n",
        "\n",
        "text = f.read()\n",
        "f.close()\n",
        "text = text.replace('\\n\\n', 'ABSTARCT')\n",
        "text = text.replace('\\n', 'BLANK')\n",
        "text = text.replace('Conflict', 'DOI:')\n",
        "text = text.replace('PMID:', 'DOI:')\n",
        "text = text.replace('© ', 'DOI:')\n",
        "text = text.replace('Copyright ©', 'DOI:')\n",
        "text = text.replace('Erratum in', 'DOI:')\n",
        "text = text.replace('Comment in', 'DOI:')\n",
        "text = text.replace('Copyright ©', 'DOI:')\n",
        "text = text.replace('Thieme. All rights reserved.','DOI:')\n",
        "\n",
        "try:\n",
        "    marker1 = 'Author information:'\n",
        "    marker2 = 'DOI:'\n",
        "\n",
        "\n",
        "    regexPattern = marker1 + '(.+?)' + marker2\n",
        "    #regexCommentPattern = \"Comment (in|on) .* (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec).{15,25}\\.\"\n",
        "    abtracts = re.findall(regexPattern, text)\n",
        "\n",
        "    f = open('/content/drive/MyDrive/TextMining/Output/merged1.txt', 'w', encoding=\"utf-8\")\n",
        "    skip = False\n",
        "    output = ''\n",
        "    #counter =1\n",
        "    for abtract in abtracts:\n",
        "        #f.write(str(counter) + ':\\n').\n",
        "        #counter+=1\n",
        "        article = abtract.replace('HexAI', '\\n')\n",
        "        for line in article.split('\\n'):\n",
        "            if len(line) > 1:\n",
        "                if skip:\n",
        "                  if(line[len(line)-1] != '.'): # to skip the second, third line of author informations\n",
        "                    skip = True\n",
        "                    continue\n",
        "                  else:\n",
        "                    skip = False\n",
        "                    continue\n",
        "                if (line[0] == '(' and line[1].isdigit() and line[2].isdigit() and line[3] == ')')  or (line[0]== '(' and line[1].isdigit() and line[2] == ')') :\n",
        "                    if(line[len(line) -1] == '.'):  # to fix online author information\n",
        "                      skip = False\n",
        "                    else:\n",
        "                      skip = True\n",
        "                else:\n",
        "                    f.write(line)\n",
        "        f.write('\\n')\n",
        "    f.close()\n",
        "except AttributeError:\n",
        "    str_found = 'Nothing found between two markers'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAKE IT CLEAN COUNTER"
      ],
      "metadata": {
        "id": "8O57SRJbJSxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Aug  8 19:40:14 2023\n",
        "\n",
        "@author: user\n",
        "\"\"\"\n",
        "\n",
        "# with open('C:/Users/user/Desktop/Ahmad/Abstract_cleaned.txt', 'r', encoding=\"utf-8\") as fp:\n",
        "#     for count, line in enumerate(fp):\n",
        "#         print(\"Abstract\",count)\n",
        "#         print(line)\n",
        "\n",
        "import re\n",
        "f = open(\"/content/drive/MyDrive/TextMining/Output/merged1.txt\", 'r', encoding=\"utf-8\")\n",
        "text = f.read()\n",
        "f.close()\n",
        "text = text.replace('BLANK', '')\n",
        "\n",
        "\n",
        "try:\n",
        "    marker1 = 'ABSTARCT'\n",
        "    marker2 = 'ABSTARCT'\n",
        "    regexPattern = marker1 + '(.+?)' + marker2\n",
        "    #regexCommentPattern = \"Comment (in|on) .* (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec).{15,25}\\.\"\n",
        "    abtracts = re.findall(regexPattern, text)\n",
        "\n",
        "    f = open('/content/drive/MyDrive/TextMining/Output/merged2.txt', 'w', encoding=\"utf-8\")\n",
        "    skip = False\n",
        "    output = ''\n",
        "    #counter =1\n",
        "    for count ,abtract in  enumerate(abtracts):\n",
        "        article = abtract.replace('HexAI', '\\n')\n",
        "        for line in article.split('\\n'):\n",
        "            if len(line) > 1:\n",
        "                if skip:\n",
        "                  if(line[len(line)-1] != '.'): # to skip the second, third line of author informations\n",
        "                    skip = True\n",
        "                    continue\n",
        "                  else:\n",
        "                    skip = False\n",
        "                    continue\n",
        "                if (line[0] == '(' and line[1].isdigit() and line[2].isdigit() and line[3] == ')')  or (line[0]== '(' and line[1].isdigit() and line[2] == ')') :\n",
        "                    if(line[len(line) -1] == '.'):  # to fix online author information\n",
        "                      skip = False\n",
        "                    else:\n",
        "                      skip = True\n",
        "                else:\n",
        "                    f.write(str(count+1)+':: '+line)\n",
        "        f.write('\\n')\n",
        "    f.close()\n",
        "except AttributeError:\n",
        "    str_found = 'Nothing found between two markers'\n",
        "\n"
      ],
      "metadata": {
        "id": "rXx_UX95JtY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONVERT TO XLSX"
      ],
      "metadata": {
        "id": "PUSJmWiaJuft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Aug  8 22:42:44 2023\n",
        "\n",
        "@author: user\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# reading the given csv file\n",
        "# and creating dataframe\n",
        "abstract = pd.read_csv(\"/content/drive/MyDrive/TextMining/Chronic_Pain/merged2.txt\", index_col = False, encoding=\"utf-8\", delimiter = \"::\", engine=\"python\", header = None)\n",
        "abstract.columns = ['Index','Abstracts']\n",
        "output_file = \"/content/drive/MyDrive/TextMining/Chronic_Pain/merged3.xlsx\"\n",
        "\n",
        "abstract.to_excel(output_file, index=False, header=True, engine='openpyxl')"
      ],
      "metadata": {
        "id": "t8PuBSxUJvIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c88bebb0-b3dc-4a4a-f88b-2050fc564f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c6f894e1e959>:13: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
            "  abstract = pd.read_csv(\"/content/drive/MyDrive/TextMining/Chronic_Pain/merged2.txt\", index_col = False, encoding=\"utf-8\", delimiter = \"::\", engine=\"python\", header = None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONVERT TO JSON"
      ],
      "metadata": {
        "id": "NOEhzL9qJzcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Aug  8 21:39:47 2023\n",
        "\n",
        "@author: user\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "\n",
        "dict1 = {}\n",
        "\n",
        "# fields in the sample file\n",
        "fields = ['Index','Abstract']\n",
        "\n",
        "with  open('/content/drive/MyDrive/TextMining/Chronic_Pain/merged2.txt', 'r', encoding=\"utf-8\") as in_file:\n",
        "\n",
        "    # count variable\n",
        "    n = 1\n",
        "    for line in in_file:\n",
        "        # reading line by line from the text file\n",
        "        description = list( line.strip().split('::'))\n",
        "\n",
        "        # for output see below\n",
        "       # print(description)\n",
        "\n",
        "        #for automatic creation of index for each abstarct\n",
        "        sno ='Abstract'+str(n)\n",
        "        # loop variable\n",
        "        i = 0\n",
        "        # intermediate dictionary\n",
        "        dict2 = {}\n",
        "        while i<len(fields):\n",
        "\n",
        "                # creating dictionary for each employee\n",
        "                dict2[fields[i]]= description[i-1]\n",
        "                i = i + 1\n",
        "\n",
        "        # appending the record of each employee to\n",
        "        # the main dictionary\n",
        "        dict1[sno]= dict2\n",
        "        n = n + 1\n",
        "\n",
        "\n",
        "# creating json file\n",
        "out_file = open('/content/drive/MyDrive/TextMining/Chronic_Pain/merged.json', 'w', encoding=\"utf-8\")\n",
        "json.dump(dict1, out_file, indent=len(description))\n",
        "out_file.close()"
      ],
      "metadata": {
        "id": "gO5TdeQOJ3Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONVERT TO CSV"
      ],
      "metadata": {
        "id": "7eQycXu_J4IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Aug  7 22:20:18 2023\n",
        "\n",
        "@author: user\n",
        "\"\"\"\n",
        "# importing pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# reading the given csv file\n",
        "# and creating dataframe\n",
        "account = pd.read_csv(r\"/content/drive/MyDrive/TextMining/Output/merged2.txt\", index_col = False, encoding=\"utf-8\", delimiter = '::',engine='python',header = None)\n",
        "account.columns = ['Index','Abstracts']\n",
        "\n",
        "# store dataframe into csv file\n",
        "account.to_csv('/content/drive/MyDrive/TextMining/Output/merged.csv', encoding='utf-8', index= None)"
      ],
      "metadata": {
        "id": "5EthuK4PJ6iF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1611fc-0baa-4008-813e-7d21c8d3cbbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-8beda749fd8b>:12: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
            "  account = pd.read_csv(r\"/content/drive/MyDrive/TextMining/Output/merged2.txt\", index_col = False, encoding=\"utf-8\", delimiter = '::',engine='python',header = None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all CSVs\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Set the path to the folder containing CSV files\n",
        "folder_path = '/content/drive/MyDrive/MajorProject/Mergefiles'\n",
        "\n",
        "# Get a list of all CSV files in the folder\n",
        "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
        "\n",
        "# Check if there are any CSV files\n",
        "if not csv_files:\n",
        "    print(\"No CSV files found in the folder.\")\n",
        "else:\n",
        "    # Initialize an empty DataFrame to store the merged data\n",
        "    merged_data = pd.DataFrame()\n",
        "\n",
        "    # Loop through each CSV file and append its data to the merged_data DataFrame\n",
        "    for csv_file in csv_files:\n",
        "        file_path = os.path.join(folder_path, csv_file)\n",
        "        df = pd.read_csv(file_path)\n",
        "        merged_data = merged_data.append(df, ignore_index=True)\n",
        "\n",
        "    # Save the merged data to a new CSV file\n",
        "    output_file = '/content/drive/MyDrive/MajorProject/final.csv'\n",
        "    merged_data.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"Merged data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZWy-1cvFsQS",
        "outputId": "227c4e37-7f15-495f-98c5-c6ebbcdf1bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d565d977e8d5>:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  merged_data = merged_data.append(df, ignore_index=True)\n",
            "<ipython-input-2-d565d977e8d5>:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  merged_data = merged_data.append(df, ignore_index=True)\n",
            "<ipython-input-2-d565d977e8d5>:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  merged_data = merged_data.append(df, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged data saved to /content/drive/MyDrive/MajorProject/final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # extract first 100 records\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# # Assuming 'your_file.csv' is the name of your CSV file\n",
        "# file_path = '/content/drive/MyDrive/Text_Mining/Ashu/final.csv'\n",
        "\n",
        "# # Use pandas to read the CSV file\n",
        "# df = pd.read_csv(file_path)\n",
        "\n",
        "# # Extract the first 100 records\n",
        "# first_100_records = df.head(100)\n",
        "\n",
        "# # If you want to save the first 100 records to a new CSV file\n",
        "# first_100_records.to_csv('/content/drive/MyDrive/Text_Mining/Ashu/100.csv', index=False)"
      ],
      "metadata": {
        "id": "R-Ie_gAJB34B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}